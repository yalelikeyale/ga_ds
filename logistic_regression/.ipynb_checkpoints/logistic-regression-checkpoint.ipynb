{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    " \n",
    "# Logistic Regression\n",
    " \n",
    "_Authors: Multiple_\n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "- Recall how to perform linear regression in scikit-learn.\n",
    "- Demonstrate why logistic regression is a better alternative for classification than linear regression.\n",
    "- Understand the concepts of probability, odds, e, log, and log-odds in relation to machine learning.\n",
    "- Explain how logistic regression works.\n",
    "- Interpret logistic regression coefficients.\n",
    "- Use logistic regression with categorical features.\n",
    "- Compare logistic regression with other models.\n",
    "- Utilize different metrics for evaluating classifier models.\n",
    "- Construct a confusion matrix based on predicted classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Refresher: Fitting and Visualizing a Linear Regression Using scikit-learn](#refresher-fitting-and-visualizing-a-linear-regression-using-scikit-learn)\n",
    "- [Refresher: Interpreting Linear Regression Coefficients](#refresher-interpreting-linear-regression-coefficients)\n",
    "- [Predicting a Categorical Response](#predicting-a-categorical-response)\n",
    "- [Using Logistic Regression for Classification](#using-logistic-regression-for-classification)\n",
    "- [Probability, e, Log, and Log Odds](#probability-odds-e-log-and-log-odds)\n",
    "\t- [Understanding e and the Natural Logarithm](#understanding-e-and-the-natural-logarithm)\n",
    "\t- [Log Odds](#the-log-odds)\n",
    "- [What Is Logistic Regression?](#what-is-logistic-regression)\n",
    "- [Interpreting Logistic Regression Coefficients](#interpreting-logistic-regression-coefficients)\n",
    "- [Using Logistic Regression With Categorical Features](#using-logistic-regression-with-categorical-features)\n",
    "- [Comparing Logistic Regression to Other Models](#comparing-logistic-regression-to-other-models)\n",
    "- [Advanced Classification Metrics](#advanced-classification-metrics)\n",
    "\t- [Accuracy, True Positive Rate, and False Negative Rate](#accuracy-true-positive-rate-and-false-negative-rate)\n",
    "\t- [The Accuracy Paradox](#the-accuracy-paradox)\n",
    "- [OPTIONAL: How Many Samples Are Needed?](#samples)\n",
    "- [Lesson Review](#lesson-review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this lesson we learn about Logistic Regression, or what is sometimes referred to as Logistic Classification.\n",
    "\n",
    "\"How can a model be both a Regression and a Classification?\" you may ask.  \n",
    "\n",
    "\n",
    "## Discussion\n",
    "\n",
    "Have you ever had to sort objects, but everything didn't fit perfectly into groups?\n",
    "\n",
    "Example:\n",
    "- Movies/Books\n",
    "- Socks\n",
    "- Phone apps\n",
    "\n",
    "\n",
    "Logistic Regression/Classification uses elements from both the Linear Regression and the K Nearest Neighbors algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"refresher-fitting-and-visualizing-a-linear-regression-using-scikit-learn\"></a>\n",
    "## Refresher: Fitting and Visualizing a Linear Regression Using scikit-learn\n",
    "---\n",
    "\n",
    "Use Pandas to load in the glass attribute data from the UCI machine learning website. The columns are different measurements of properties of glass that can be used to identify the glass type. For detailed information on the columns in this data set, [please see the included .names file](http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sns.set(font_scale=1.5);\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "# Glass identification data set\n",
    "glass = pd.read_csv('data/glass.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change columns to something more uniform\n",
    "glass.columns = ['ri','na','mg','al','si','k','ca','ba','fe','glass_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Dictionary**\n",
    "\n",
    "- `Id`: number: 1 to 214\n",
    "- `RI`: refractive index  \n",
    "- `Na`: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)\n",
    "- `Mg`: Magnesium\n",
    "- `Al`: Aluminum\n",
    "- `Si`: Silicon\n",
    "- `K` : Potassium\n",
    "- `Ca`: Calcium\n",
    "- `Ba`: Barium\n",
    "- `Fe`: Iron\n",
    "- `Type` : Type of glass:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pretend we want to predict `ri`, and our only feature is `al`. How could we do it using machine learning?**\n",
    "\n",
    "<!--\n",
    "**Answer:** We could frame it as a regression problem, and use a linear regression model with **`al`** as the only feature and **`ri`** as the response.\n",
    "-->\n",
    "\n",
    "**How would we visualize this model?**\n",
    "<!--\n",
    "**Answer:** Create a scatter plot with **`al`** on the x-axis and **`ri`** on the y-axis, and draw the line of best fit.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/py36/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#scatter with regression line\n",
    "sns.lmplot(x='al', y='ri', data=glass);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How can we draw this plot (just the points — don't worry about the regression line) without using Seaborn?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot using Pandas\n",
    "glass.plot(kind='scatter', x='al', y='ri');\n",
    "\n",
    "# Seaborn with parameters\n",
    "#sns.lmplot(x='al', y='ri', data=glass, fit_reg=False);\n",
    "\n",
    "# Equivalent scatter plot using Matplotlib\n",
    "#plt.scatter(glass.al, glass.ri)\n",
    "#plt.xlabel('al')\n",
    "#plt.ylabel('ri')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a linear regression model to predict `ri` using scikit-learn, we will need to Import `LinearRegression` from `linear_model`.\n",
    "\n",
    "**Using `LinearRegression`, fit a model predicting `ri` from `al` (and an intercept).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear regression model (name the model \"linreg\").\n",
    "linreg = \n",
    "\n",
    "feature_cols = ['al']\n",
    "X = \n",
    "y = \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the `LinearRegression` object we have fit, create a variable that are our predictions for `ri` for each row's `al` in the data set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for all values of X and add back to the original DataFrame.\n",
    "y_pred = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the predicted `ri` against each `al` as a line.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot those predictions connected by a line (try plt.plot()).\n",
    "plt.plot(X_test.age, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note the y axis labels when comparing to seaborns plot_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot this regression line with the scatter points on the same chart.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the plots together (use a scatter and line graph).\n",
    "plt.scatter(glass.al, glass.ri)\n",
    "plt.plot(glass.al, glass.y_pred, color='red')\n",
    "plt.xlabel('al')\n",
    "plt.ylabel('ri')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"refresher-interpreting-linear-regression-coefficients\"></a>\n",
    "## Refresher: Interpreting Linear Regression Coefficients\n",
    "---\n",
    "\n",
    "Recall the simple linear regression equation is $y = \\beta_0 + \\beta_1x$\n",
    "\n",
    "$\\beta_0$ is the intercept and $\\beta_1$ is, in this case, our coefficient on the `al` predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the intercept and coefficient values from our fit `LinearRegression` object.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manually compute the predicted value of `ri` when `al=2.0` using the regression equation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute prediction for al=2 using the equation.\n",
    "linreg.intercept_ + linreg.coef_[0] * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confirm that this is the same value we would get when using the built-in `.predict()` method of the `LinearRegression` object.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute prediction for al=2 using the predict method.\n",
    "linreg.predict(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coefficient interpretation:** A 1-unit increase in `al` is associated with a ~0.0025-unit decrease in `ri`.\n",
    "\n",
    "**Intercept interpretation:** When `al = 0`, the estimated value of `ri` is 1.52194533024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"predicting-a-categorical-response\"></a>\n",
    "## Predicting a Single Categorical Response\n",
    "---\n",
    "\n",
    "Linear regression is appropriate when we want to predict the value of a continuous target/response variable, but what about when we want to predict membership in a class or category?\n",
    "\n",
    "**Examine the glass type column in the data set. What are the counts in each category?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine glass_type.\n",
    "glass.glass_type.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say these types are subdivisions of broader glass types:\n",
    "\n",
    "> **Window glass:** types 1, 2, and 3\n",
    "\n",
    "> **Household glass:** types 5, 6, and 7\n",
    "\n",
    "**Create a new `household` column that indicates whether or not a row is household glass, coded as 1 or 0, respectively.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types 1, 2, 3 are window glass.\n",
    "# Types 5, 6, 7 are household glass.\n",
    "glass['household'] = glass.glass_type.map({1:0, 2:0, 3:0, 5:1, 6:1, 7:1})\n",
    "glass.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change our task, so that we're predicting the `household` category using `al`. Let's visualize the relationship to figure out how to do this.\n",
    "\n",
    "**Make a scatter plot comparing `al` and `household`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(glass.al, glass.household)\n",
    "plt.xlabel('al')\n",
    "plt.ylabel('household');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit a new `LinearRegression` predicting `household` from `al`.**\n",
    "\n",
    "Let's draw a regression line like we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear regression model and store the predictions.\n",
    "feature_cols = ['al']\n",
    "X = glass[feature_cols] \n",
    "y = glass.household \n",
    "\n",
    "linreg = LinearRegression()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "linreg.fit(X_train, y_train)\n",
    "y_pred = linreg.predict(X_test) # prediction via Lin Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot that includes the regression line\n",
    "plt.scatter(X_test.al, y_test)\n",
    "plt.plot(X_test.al, y_pred, color='red')\n",
    "plt.xlabel('al')\n",
    "plt.ylabel('household');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If **al=3**, what class do we predict for household? **1**\n",
    "\n",
    "If **al=1.5**, what class do we predict for household? **0**\n",
    "\n",
    "We predict the 0 class for **lower** values of al, and the 1 class for **higher** values of al. What's our cutoff value? Around **al=2**, because that's where the linear regression line crosses the midpoint between predicting class 0 and class 1.\n",
    "\n",
    "Therefore, we'll say that if **household_pred >= 0.5**, we predict a class of **1**, else we predict a class of **0**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using this threshold, create a new column of our predictions for whether a row is household glass.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding np.where\n",
    "import numpy as np\n",
    "nums = np.array([5, 15, 8])\n",
    "\n",
    "# np.where returns the first value if the condition is True, and the second value if the condition is False.\n",
    "np.where(nums > 10, 'big', 'small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform household_pred to 1 or 0.\n",
    "y_pred_class = np.where(y_pred >= 0.5, 1, 0)\n",
    "y_pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot a line that shows our predictions for class membership in household vs. not.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort so we can have a continuous line\n",
    "sorted_vals = [x for _,x in sorted(zip(y_pred_class, X_test.al))]\n",
    "sorted_pred = sorted(y_pred_class)\n",
    "# Scatter plot that includes the regression line\n",
    "plt.scatter(X_test.al, y_test)\n",
    "# Plot the class predictions.\n",
    "plt.plot(sorted_vals, sorted_pred, color='red')\n",
    "plt.xlabel('med_expense')\n",
    "plt.ylabel('smoker_status');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"using-logistic-regression-for-classification\"></a>\n",
    "## Using Logistic Regression for Classification\n",
    "---\n",
    "\n",
    "Logistic regression is a more appropriate method for what we just did with a linear regression. The values output from a linear regression cannot be interpreted as probabilities of class membership since their values can be greater than 1 and less than 0. Logistic regression, on the other hand, ensures that the values output as predictions can be interpreted as probabilities of class membership.\n",
    "\n",
    "**Import the `LogisticRegression` class from `linear_model` below and fit the same regression model predicting `household` from `al`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a logistic regression model and store the class predictions.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "feature_cols = ['al']\n",
    "X = glass[feature_cols]\n",
    "y = glass.household\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test) # prediction via Log Reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the predicted class using the logistic regression as we did for the linear regression predictions above.**\n",
    "\n",
    "As you can see, the class predictions are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort so we can have a continuous line\n",
    "sorted_val = [x for _,x in sorted(zip(y_pred, X_test.al))]\n",
    "sorted_pred = sorted(y_pred)\n",
    "# Scatter plot that includes the regression line\n",
    "plt.scatter(X_test, y_test)\n",
    "# Plot the class predictions.\n",
    "plt.plot(sorted_val, sorted_pred, color='red')\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('policy-purchase');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted the predicted probabilities instead of just the class predictions, to understand how confident we are in a given prediction?\n",
    "\n",
    "**Using the built-in `.predict_proba()` function, examine the predicted probabilities for the first handful of rows of `X`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.predict_proba(X_test)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn orders the columns according to our class labels. The two-column output of `predict_proba` returns a column for each class of our `household` variable. The first column is the probability of `household=0` for a given row, and the second column is the probability of `household=1`.\n",
    "\n",
    "**Store the predicted probabilities of class=1 in its own column in the data set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the predicted probabilities of class 1.\n",
    "p_class_1 = logreg.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the predicted probabilities as a line on our plot (probability of `household=1` as `al` changes).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_val = [x for _,x in sorted(zip(y_pred, X_test.al))]\n",
    "p_class_1 = sorted(p_class_1)\n",
    "\n",
    "# Plot the predicted probabilities.\n",
    "plt.scatter(X_test.al, y_test)\n",
    "plt.plot(sorted_val, p_class_1, color='red')\n",
    "plt.xlabel('al')\n",
    "plt.ylabel('household');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine some example predictions.\n",
    "print(logreg.predict_proba(1))\n",
    "print(logreg.predict_proba(2))\n",
    "print(logreg.predict_proba(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "\n",
    "- Build and train a logistic regression model.\n",
    "- Select 2 features for you X\n",
    "- y will remain the same `glass.household`\n",
    "- Evaluate the model with `model.score` on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"probability-odds-e-log-and-log-odds\"></a>\n",
    "## Probability, e, Log, and Log Odds\n",
    "---\n",
    "\n",
    "To understand how logistic regression predicts the probability of class membership we need to start by understanding the relationship between probability, odds ratios, and log odds ratios. This is because logistic regression predicts log odds and so reading log odds is extremely useful for interpreting logistic regression.\n",
    "\n",
    "$$probability = \\frac {one\\ outcome} {all\\ outcomes}$$\n",
    "\n",
    "$$odds = \\frac {one\\ outcome} {all\\ other\\ outcomes}$$\n",
    "\n",
    "It is often useful to think of the numeric odds as a ratio. For example, 5/1 = 5 odds is \"5 to 1\" -- five wins for every one loss (e.g. of six total plays). 2/3 odds means \"2 to 3\" -- two wins for every three losses (e.g. of five total plays).\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Dice roll of 1: probability = 1/6, odds = 1/5\n",
    "- Even dice roll: probability = 3/6, odds = 3/3 = 1\n",
    "- Dice roll less than 5: probability = 4/6, odds = 4/2 = 2\n",
    "\n",
    "$$odds = \\frac {probability} {1 - probability}$$\n",
    "\n",
    "$$probability = \\frac {odds} {1 + odds}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As an example we can create a table of probabilities vs. odds, as seen below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table of probability versus odds.\n",
    "table = pd.DataFrame({'probability':[0.1, 0.2, 0.25, 0.5, 0.6, 0.8, 0.9]})\n",
    "table['odds'] = table.probability / (1 - table.probability)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"understanding-e-and-the-natural-logarithm\"></a>\n",
    "### Understanding e and the Natural Logarithm\n",
    "\n",
    "What is e? It is the base rate of growth shared by all continually growing processes:\n",
    "\n",
    "e is the irrational base of the natural log `ln`.\n",
    "\n",
    "- 2.718281828459"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential function: e^1\n",
    "e = np.exp(1)\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a (natural) log? It gives you the time needed to reach a certain level of growth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time needed to grow 1 unit to 2.718 units\n",
    "# ln e = 1\n",
    "np.log(2.718281828459) # very close to previous value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also the inverse of the exponential function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e^5\n",
    "np.exp(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.exp(5)\n",
    "2.7182818**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the log of the exponential returns back to original input\n",
    "np.log(np.exp(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take one of our odds from out table and walk through how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odds = 0.25\n",
    "# ln 0.25 = -1.38629436\n",
    "np.log(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(e**-1.3862943611198906)\n",
    "print(np.exp(-1.3862943611198906))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"the-log-odds\"></a>\n",
    "\n",
    "When we take the logarithm of the odds, we get what is known as the **log odds**. This may seem like an arbitrary transformation, but it has an important property: The log odds has the range $[-\\infty, \\infty]$. This is not true for the odds ratio, which can never be a negative number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add log odds to the table.\n",
    "table['logodds'] = np.log(table['odds'])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(54) == np.log(9) + np.log(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what-is-logistic-regression\"></a>\n",
    "## What Is Logistic Regression?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression: Continuous response is modeled as a linear combination of the features.\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1x$$\n",
    "\n",
    "Logistic regression: Log odds of a categorical response being \"true\" (1) is modeled as a linear combination of the features.\n",
    "\n",
    "$$\\log \\left({p\\over 1-p}\\right) = \\beta_0 + \\beta_1x$$\n",
    "\n",
    "This is called the logit function.\n",
    "\n",
    "The equation can be rearranged into the logistic function.\n",
    "\n",
    "$$\\hat{p} = \\frac{e^{\\beta_0 + \\beta_1x}} {1 + e^{\\beta_0 + \\beta_1x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words:\n",
    "\n",
    "- Logistic regression outputs the probabilities of a specific class.\n",
    "- Those probabilities can be converted into class predictions.\n",
    "\n",
    "The logistic function has some nice properties:\n",
    "\n",
    "- Takes on an \"s\" shape\n",
    "- Output is bounded by 0 and 1\n",
    "\n",
    "We have covered how this works for binary classification problems (two response classes). But what about multi-class classification problems (more than two response classes)?\n",
    "\n",
    "- The most common solution for classification models is \"one-vs-all\" (also known as \"one-vs-rest\"): Decompose the problem into multiple binary classification problems.\n",
    "- Multinomial logistic regression, on the other hand, can solve this as a single problem, but how this works is beyond the scope of this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"interpreting-logistic-regression-coefficients\"></a>\n",
    "## Interpreting Logistic Regression Coefficients\n",
    "\n",
    "---\n",
    "\n",
    "Logistic regression coefficients are not as immediately interpretable as the coefficients from a linear regression. To interpret the coefficients we need to remember how the formulation for logistic regression differs from linear regression.\n",
    "\n",
    "**First let's plot our logistic regression predicted probability line again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted probabilities again.\n",
    "plt.scatter(X_test.al, y_test)\n",
    "plt.plot(sorted_val, p_class_1, color='red')\n",
    "plt.xlabel('al')\n",
    "plt.ylabel('household');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember:\n",
    "\n",
    "$$\\log \\left({p\\over 1-p}\\right) = \\beta_0 + \\beta_1x$$\n",
    "\n",
    "That means we'll get out the log odds if we compute the intercept plus the coefficient times a value for `al`.\n",
    "\n",
    "**Compute the log odds of `household` when `al=2`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted log odds for al=2 using the equation.\n",
    "logodds = logreg.intercept_ + logreg.coef_[0] * 2\n",
    "logodds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the log odds, we will need to go through the process of converting these log odds to probability.\n",
    "\n",
    "**Convert the log odds to odds, then the odds to probability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert log odds to odds.\n",
    "odds = np.exp(logodds)\n",
    "odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert odds to probability.\n",
    "prob = odds / (1 + odds)\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This finally gives us the predicted probability of `household=1` when `al=2`. You can confirm this is the same as the value you would get out of the `.predict_proba()` method of the sklearn object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probability for al=2 using the predict_proba method.\n",
    "logreg.predict_proba(2)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the coefficient for al.\n",
    "list(zip(feature_cols, logreg.coef_[0]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the intercept.\n",
    "logreg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** A 1-unit increase in `al` is associated with a 2.01-unit increase in the log odds of `household`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increasing al by 1 (so that al=3)\n",
    "logodds = -3.49257043 + 1.705040473427377*3\n",
    "odds = np.exp(logodds)\n",
    "prob = odds/(1 + odds)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probability for al=3 using the predict_proba method.\n",
    "logreg.predict_proba(3)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bottom line:** Positive coefficients increase the log odds of the response (and thus increase the probability), and negative coefficients decrease the log odds of the response (and thus decrease the probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the intercept.\n",
    "logreg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intercept interpretation:** For an `al` value of 0, the log-odds of `household` is -3.49257043."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert log odds to probability.\n",
    "logodds = logreg.intercept_\n",
    "odds = np.exp(logodds)\n",
    "prob = odds/(1 + odds)\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That makes sense from the plot above, because the probability of `household=1` should be very low for such a low `al` value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Logistic regression beta values](./assets/logistic_betas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the $\\beta_0$ value shifts the curve horizontally, whereas changing the $\\beta_1$ value changes the slope of the curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparing-logistic-regression-to-other-models\"></a>\n",
    "## Comparing Logistic Regression to Other Models\n",
    "---\n",
    "\n",
    "Advantages of logistic regression:\n",
    "\n",
    "- Highly interpretable (if you remember how).\n",
    "- Model training and prediction are fast.\n",
    "- No tuning is required (excluding regularization).\n",
    "- Features don't need scaling.\n",
    "- Can perform well with a small number of observations.\n",
    "- Outputs well-calibrated predicted probabilities.\n",
    "\n",
    "Disadvantages of logistic regression:\n",
    "\n",
    "- Presumes a linear relationship between the features and the log odds of the response.\n",
    "- Performance is (generally) not competitive with the best supervised learning methods.\n",
    "- Can't automatically learn feature interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"advanced-classification-metrics\"></a>\n",
    "## Advanced Classification Metrics\n",
    "\n",
    "---\n",
    "\n",
    "When we evaluate the performance of a logistic regression (or any classifier model), the standard metric to use is accuracy: How many class labels did we guess correctly? However, accuracy is only one of several metrics we could use when evaluating a classification model.\n",
    "\n",
    "$$Accuracy = \\frac{total~predicted~correct}{total~predicted}$$\n",
    "\n",
    "Accuracy alone doesn’t always give us a full picture.\n",
    "\n",
    "If we know a model is 75% accurate, it doesn’t provide any insight into why the 25% was wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a binary classification problem where we have 165 observations/rows of people who are either smokers or nonsmokers.\n",
    "\n",
    "<table style=\"border: none\">\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none; vertical-align: bottom\">n = 165</td>\n",
    "    <td style=\"\"><b>Predicted: No</b></td>\n",
    "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: No</b></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: Yes</b></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "</tr>\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none\"></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 60 in class 0, nonsmokers, and 105 observations in class 1, smokers\n",
    "<table style=\"border: none\">\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none; vertical-align: bottom\">n = 165</td>\n",
    "    <td style=\"\"><b>Predicted: No</b></td>\n",
    "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: No</b></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "    <td style=\"text-align: center\">60</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: Yes</b></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "    <td style=\"text-align: center\">105</td>\n",
    "</tr>\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none\"></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 55 predictions of class, predicted as nonsmokers, and 110 of class 1, predicted to be smokers.\n",
    "\n",
    "<table style=\"border: none\">\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none; vertical-align: bottom\">n = 165</td>\n",
    "    <td style=\"\"><b>Predicted: No</b></td>\n",
    "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: No</b></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "    <td style=\"text-align: center\">60</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: Yes</b></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "    <td style=\"text-align: center\">105</td>\n",
    "</tr>\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none\"></td>\n",
    "    <td style=\"text-align: center\">55</td>\n",
    "    <td style=\"text-align: center\">110</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **True positives (TP):** These are cases in which we predicted yes (smokers), and they actually are smokers.\n",
    "- **True negatives (TN):** We predicted no, and they are nonsmokers.\n",
    "- **False positives (FP):** We predicted yes, but they were not actually smokers. (This is also known as a \"Type I error.\")\n",
    "- **False negatives (FN):** We predicted no, but they are smokers. (This is also known as a \"Type II error.\")\n",
    "<table style=\"border: none\">\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none; vertical-align: bottom\">n = 165</td>\n",
    "    <td style=\"\"><b>Predicted: No</b></td>\n",
    "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: No</b></td>\n",
    "    <td style=\"text-align: center\">TN = 50</td>\n",
    "    <td style=\"text-align: center\">FP = 10</td>\n",
    "    <td style=\"text-align: center\">60</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: Yes</b></td>\n",
    "    <td style=\"text-align: center\">FN = 5</td>\n",
    "    <td style=\"text-align: center\">TP = 100</td>\n",
    "    <td style=\"text-align: center\">105</td>\n",
    "</tr>\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none\"></td>\n",
    "    <td style=\"text-align: center\">55</td>\n",
    "    <td style=\"text-align: center\">110</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorize these as TP, TN, FP, or FN:**\n",
    "\n",
    "Try not to look at the answers above.\n",
    "    \n",
    "- We predict nonsmoker, but the person is a smoker.\n",
    "- We predict nonsmoker, and the person is a nonsmoker.\n",
    "- We predict smoker and the person is a smoker.\n",
    "- We predict smoker and the person is a nonsmoker.\n",
    "\n",
    "<!--ANSWER\n",
    "- FN\n",
    "- TN\n",
    "- TP\n",
    "- FP\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"accuracy-true-positive-rate-and-false-negative-rate\"></a>\n",
    "### Accuracy, True Positive Rate, and False Negative Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy:** Overall, how often is the classifier correct?\n",
    "\n",
    "<span>\n",
    "    (<span style=\"color: green\">TP</span>+<span style=\"color: red\">TN</span>)/<span style=\"color: blue\">total</span> = (<span style=\"color: green\">100</span>+<span style=\"color: red\">50</span>)/<span style=\"color: blue\">165</span> = 0.91\n",
    "</span>\n",
    "\n",
    "<table style=\"border: none\">\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none; vertical-align: bottom; color: blue\">n = 165</td>\n",
    "    <td style=\"\"><b>Predicted: No</b></td>\n",
    "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: No</b></td>\n",
    "    <td style=\"text-align: center; background-color: red\">TN = 50</td>\n",
    "    <td style=\"text-align: center\">FP = 10</td>\n",
    "    <td style=\"text-align: center\">60</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: Yes</b></td>\n",
    "    <td style=\"text-align: center\">FN = 5</td>\n",
    "    <td style=\"text-align: center; background-color: green\">TP = 100</td>\n",
    "    <td style=\"text-align: center\">105</td>\n",
    "</tr>\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none\"></td>\n",
    "    <td style=\"text-align: center\">55</td>\n",
    "    <td style=\"text-align: center\">110</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True positive rate (TPR)** asks, “Out of all of the target class labels, how many were accurately predicted to belong to that class?”\n",
    "\n",
    "For example, given a medical exam that tests for cancer, how often does it correctly identify patients with cancer?\n",
    "\n",
    "<span>\n",
    "<span style=\"color: green\">TP</span>/<span style=\"color: blue\">actual yes</span> = <span style=\"color: green\">100</span>/<span style=\"color: blue\">105</span> = 0.95\n",
    "</span>\n",
    "\n",
    "<table style=\"border: none\">\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none; vertical-align: bottom\">n = 165</td>\n",
    "    <td style=\"\"><b>Predicted: No</b></td>\n",
    "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: No</b></td>\n",
    "    <td style=\"text-align: center\">TN = 50</td>\n",
    "    <td style=\"text-align: center\">FP = 10</td>\n",
    "    <td style=\"text-align: center\">60</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: Yes</b></td>\n",
    "    <td style=\"text-align: center\">FN = 5</td>\n",
    "    <td style=\"text-align: center;background-color: green\">TP = 100</td>\n",
    "    <td style=\"text-align: center;color: blue\">105</td>\n",
    "</tr>\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none\"></td>\n",
    "    <td style=\"text-align: center\">55</td>\n",
    "    <td style=\"text-align: center\">110</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False positive rate (FPR)** asks, “Out of all items not belonging to a class label, how many were predicted as belonging to that target class label?”\n",
    "\n",
    "For example, given a medical exam that tests for cancer, how often does it trigger a “false alarm” by incorrectly saying a patient has cancer?\n",
    "\n",
    "<span>\n",
    "<span style=\"color: orange\">FP</span>/<span style=\"color: blue\">actual no</span> = <span style=\"color: orange\">10</span>/<span style=\"color: blue\">60</span> = 0.17\n",
    "</span>\n",
    "\n",
    "<table style=\"border: none\">\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none; vertical-align: bottom\">n = 165</td>\n",
    "    <td style=\"\"><b>Predicted: No</b></td>\n",
    "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: No</b></td>\n",
    "    <td style=\"text-align: center\">TN = 50</td>\n",
    "    <td style=\"text-align: center;background-color: orange\">FP = 10</td>\n",
    "    <td style=\"text-align: center;color:blue\">60</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: Yes</b></td>\n",
    "    <td style=\"text-align: center\">FN = 5</td>\n",
    "    <td style=\"text-align: center\">TP = 100</td>\n",
    "    <td style=\"text-align: center\">105</td>\n",
    "</tr>\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none\"></td>\n",
    "    <td style=\"text-align: center\">55</td>\n",
    "    <td style=\"text-align: center\">110</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can you see that we might weigh TPR AND FPR differently depending on the situation?**\n",
    "\n",
    "- Give an example when we care about TPR, but not FPR.\n",
    "- Give an example when we care about FPR, but not TPR.\n",
    "\n",
    "<!--\n",
    "ANSWER:\n",
    "- During an initial medical diagnosis, we want to be sensitive. We want initial screens to come up with a lot of true positives, even if we get a lot of false positives.\n",
    "- If we are doing spam detection, we want to be precise. Anything that we remove from an inbox must be spam, which may mean accepting fewer true positives.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More Trade-Offs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true positive and false positive rates gives us a much clearer picture of where predictions begin to fall apart.\n",
    "\n",
    "This allows us to adjust our models accordingly.\n",
    "\n",
    "**Below we will load in some data on admissions to college.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model, model_selection, metrics\n",
    "\n",
    "admissions = pd.read_csv('data/admissions.csv')\n",
    "admissions = admissions.dropna()\n",
    "# Get dummy variables for prestige.\n",
    "admissions = admissions.join(pd.get_dummies(admissions['prestige'], prefix='prestige'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can predict the `admit` class from `gre` and use a train-test split to evaluate the performance of our model on a held-out test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = admissions[['gre']]\n",
    "y = admissions['admit']\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=1)\n",
    "logit_simple = linear_model.LogisticRegression(max_iter=1000, class_weight={0:0.9, 1: 0.1}).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall that our \"baseline\" accuracy is the proportion of the majority class label.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train[y_train == y_train.value_counts().index[0]]) / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is our accuracy on the test set?\n",
    "print(np.mean(y_test == logit_simple.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a confusion matrix of predictions on our test set using `metrics.confusion_matrix`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probability predictions.\n",
    "logit_pred_proba = logit_simple.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_true=y_test, y_pred=logit_pred_proba > .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer the following:**\n",
    "\n",
    "- What is our accuracy on the test set?\n",
    "- True positive rate?\n",
    "- False positive rate?\n",
    "\n",
    "<!--\n",
    "ANSWER: This will depend on the data:\n",
    "Accuracy: 64%\n",
    "TPR: 0\n",
    "FPR: 0\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good classifier would have a true positive rate approaching 1 and a false positive rate approaching 0.\n",
    "\n",
    "In our admissions problem, this model would accurately predict all of the admissions as admissions and not accidentally predict any of the nonadmissions as admissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trading True Positives and True Negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, and with respect to the underlying assumptions of logistic regression, we predict a positive class when the probability of the class is greater than .5 and predict a negative class otherwise.\n",
    "\n",
    "What if we decide to use .3 as a threshold for picking the positive class? Is that even allowed?\n",
    "\n",
    "This turns out to be a useful strategy. By setting a lower probability threshold we will predict more positive classes. Which means we will predict more true positives, but fewer true negatives.\n",
    "\n",
    "Making this trade-off is important in applications that have imbalanced penalties for misclassification.\n",
    "\n",
    "The most popular example is medical diagnostics, where we want as many true positives as feasible. For example, if we are diagnosing cancer we prefer to have false positives, predict a cancer when there is no cancer, that can be later corrected with a more specific test.\n",
    "\n",
    "We do this in machine learning by setting a low threshold for predicting positives which increases the number of true positives and false positives, but allows us to balance the the costs of being correct and incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can vary the classification threshold for our model to get different predictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_true=y_test, y_pred=logit_pred_proba > .095)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Accuracy Paradox\n",
    "\n",
    "Accuracy is a very intuitive metric — it's a lot like an exam score where you get total correct/total attempted. However, accuracy is often a poor metric in application. There are many reasons for this:\n",
    "- Imbalanced problems problems with 95% positives in the baseline will have 95% accuracy even with no predictive power.\n",
    "  - This is the paradox; pursuing accuracy often means predicting the most common class rather than doing the most useful work.\n",
    "- Applications often have uneven penalties and rewards for true positives and false positives.\n",
    "- Ranking predictions in the correct order be more important than getting them correct.\n",
    "- In many case we need to know the exact probability of a positives and negatives.\n",
    "  - To calculate an expected return.\n",
    "  - To triage observations that are borderline positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some of the most useful metrics for addressing these problems are:**\n",
    "    \n",
    "- **Classification accuracy/error**\n",
    "  - Classification accuracy is the percentage of correct predictions (higher is better).\n",
    "  - Classification error is the percentage of incorrect predictions (lower is better).\n",
    "  - Easiest classification metric to understand.\n",
    "- **Confusion matrix**\n",
    "  - Gives you a better understanding of how your classifier is performing.\n",
    "  - Allows you to calculate sensitivity, specificity, and many other metrics that might match your business objective better than accuracy.\n",
    "  - Precision and recall are good for balancing misclassification costs.\n",
    "- **ROC curves and area under a curve (AUC)**\n",
    "  - Good for ranking and prioritization problems.\n",
    "  - Allows you to visualize the performance of your classifier across all possible classification thresholds, thus helping you to choose a threshold that appropriately balances sensitivity and specificity.\n",
    "  - Still useful when there is high class imbalance (unlike classification accuracy/error).\n",
    "  - Harder to use when there are more than two response classes.\n",
    "- **Log loss**\n",
    "  - Most useful when well-calibrated predicted probabilities are important to your business objective.\n",
    "    - Expected value calculations\n",
    "    - Triage\n",
    "\n",
    "The good news is that these are readily available in Python and R, and are usually easy to calculate once you know about them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"samples\"></a>\n",
    "## OPTIONAL: How Many Samples Are Needed?\n",
    "\n",
    "We often ask how large our data set should be to achieve a reasonable logistic regression result. Below, a few methods will be introduced for determining how accurate the resulting model will be.\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "**Quick:** At least 100 samples total. At least 10 samples per feature.\n",
    "\n",
    "**Formula method:**\n",
    "1. Find the proportion $p$ of positive cases and negative cases. Take the smaller of the two. \n",
    "    - Ideally, you want 50/50 for a proportion of 0.5.\n",
    "    - Example: Suppose we are predicting \"male\" or \"female\". Our data is 80% male, 20% female. \n",
    "        - So, we choose the proportion $p = 0.2$ since it is smaller.\n",
    "\n",
    "2. Find the number of independent variables $k$.\n",
    "    - Example: We are predicting gender based on the last letter of the first name, giving us 26 indicator columns for features. So, $k = 26$.\n",
    "\n",
    "3. Let the minimum number of cases be $N = \\frac{10k}{p}$. The minimum should always be set to at least $100$.\n",
    "    - Example: Here, $N = 10*26 / 0.2 = 1300$. So, we would need 1300 names (supposing 80% are male).\n",
    "\n",
    "\n",
    "Both methods from: Long, J. S. (1997). *Regression Models for Categorical and Limited Dependent Variables*. Thousand Oaks, CA: Sage Publications.\n",
    "\n",
    "\n",
    "### Statistical Testing\n",
    "\n",
    "Logistic regression is one of the few machine learning models where we can obtain comprehensive statistics. By performing hypothesis testing, we can understand whether we have sufficient data to make strong conclusions about individual coefficients and the model as a whole. A very popular Python library which gives you these statistics with just a few lines of code is [statsmodels](http://www.statsmodels.org/dev/index.html).\n",
    "\n",
    "\n",
    "### Power Analysis\n",
    "\n",
    "As you may suspect, many factors affect how statistically significant the results of a logistic regression are. The art of estimating the sample size to detect an effect of a given size with a given degree of confidence is called power analysis.\n",
    "\n",
    "Some factors that influence the accuracy of our resulting model are:\n",
    "\n",
    "+ Desired statistical significance (p-value)\n",
    "+ Magnitude of the effect\n",
    "    - It is more difficult to distinguish a small effect from noise. So, more data would be required!\n",
    "+ Measurement precision\n",
    "+ Sampling error\n",
    "    - An effect is more difficult to detect in a smaller sample.\n",
    "+ Experimental design\n",
    "\n",
    "So, many factors, in addition to the number of samples, contribute to the resulting statistical power. Hence, it is difficult to give an absolute number without a more comprehensive analysis. This analysis is out of the scope of this lesson, but it is important to understand some of the factors that affect confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Review\n",
    "- **Logistic regression**\n",
    "  - What kind of machine learning problems does logistic regression address?\n",
    "  - What do the coefficients in a logistic regression represent? How does the interpretation differ from ordinary least squares? How is it similar?\n",
    "  \n",
    "- **The confusion matrix**\n",
    "  - How do true positive rate and false positive rate help explain accuracy?\n",
    "  - Why might one classification metric be more important to tune than another? Give an example of a business problem or project where this would be the case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
