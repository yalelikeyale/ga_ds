{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Linear Regression\n",
    "\n",
    "_Authors: Kevin Markham (Washington, D.C.), Ed Podojil (New York City)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Learning Objectives\n",
    "- Define data modeling and simple linear regression.\n",
    "- Build a linear regression model using a data set that meets the linearity assumption using the scikit-learn library.\n",
    "- Understand and identify multicollinearity in a multiple regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduce the Bikeshare Data Set](#introduce-the-bikeshare-dataset)\n",
    "\t- [Read in the  Capital Bikeshare Data](#read-in-the--capital-bikeshare-data)\n",
    "\t- [Visualizing the Data](#visualizing-the-data)\n",
    "- [Linear Regression Basics](#linear-regression-basics)\n",
    "\t- [Form of Linear Regression](#form-of-linear-regression)\n",
    "- [Overview of Supervised Learning](#overview-of-supervised-learning)\n",
    "\t- [Benefits and Drawbacks of scikit-learn](#benefits-and-drawbacks-of-scikit-learn)\n",
    "\t- [Requirements for Working With Data in scikit-learn](#requirements-for-working-with-data-in-scikit-learn)\n",
    "\t- [Building a Linear Regression Model in sklearn](#building-a-linear-regression-model-in-sklearn)\n",
    "\t- [scikit-learn's Four-Step Modeling Pattern](#scikit-learns--step-modeling-pattern)\n",
    "- [Build a Linear Regression Model](#build-a-linear-regression-model)\n",
    "- [Using the Model for Prediction](#using-the-model-for-prediction)\n",
    "\t- [Does the Scale of the Features Matter?](#does-the-scale-of-the-features-matter)\n",
    "- [Work With Multiple Features](#work-with-multiple-features)\n",
    "\t- [Visualizing the Data (Part 2)](#visualizing-the-data-part-)\n",
    "\t- [Adding More Features to the Model](#adding-more-features-to-the-model)\n",
    "- [What Is Multicollinearity?](#what-is-multicollinearity)\n",
    "- [How to Select a Model](#how-to-select-a-model)\n",
    "\t- [Feature Selection](#feature-selection)\n",
    "\t- [Evaluation Metrics for Regression Problems](#evaluation-metrics-for-regression-problems)\n",
    "\t- [Comparing Models With Train/Test Split and RMSE](#comparing-models-with-traintest-split-and-rmse)\n",
    "\t- [Comparing Testing RMSE With Null RMSE](#comparing-testing-rmse-with-null-rmse)\n",
    "- [Feature Engineering to Improve Performance](#feature-engineering-to-improve-performance)\n",
    "\t- [Handling Categorical Features](#handling-categorical-features)\n",
    "\t- [Feature Engineering](#feature-engineering)\n",
    "- [Bonus Material: Regularization](#bonus-material-regularization)\n",
    "\t- [How Does Regularization Work?](#how-does-regularization-work)\n",
    "\t- [Lasso and Ridge Path Diagrams](#lasso-and-ridge-path-diagrams)\n",
    "\t- [Advice for Applying Regularization](#advice-for-applying-regularization)\n",
    "\t- [Ridge Regression](#ridge-regression)\n",
    "- [Comparing Linear Regression With Other Models](#comparing-linear-regression-with-other-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"introduce-the-bikeshare-dataset\"></a>\n",
    "## Introduce the Bikeshare Data Set\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working with a data set from Capital Bikeshare that was used in a Kaggle competition ([data dictionary](https://www.kaggle.com/c/bike-sharing-demand/data)).\n",
    "\n",
    "The objective of the competition is to predict total ridership of Capital Bikeshare in any given hour.\n",
    "\n",
    "Demand forecasting is a common data science application. If we can predict the quantity of demand, total ridership in a given hour, we can create analytical tools to improve the bikeshare system. \n",
    "Some applications would be:\n",
    "* Find where to site new bikeshare stations and know how large of a station to build.\n",
    "* Calculate the expected wear and tear on bikes and what the replacement costs will be.\n",
    "* Use a slightly different research design to forecast full and empty stations and send a service vehicle to \"rebalance\" the bikes from one station to another, as sometimes bikeshare stations have no bikes or are completely full and prevent use of the station.\n",
    "\n",
    "Businesses aren't new to demand forecasting, but older methods suffered from poor predictions at atypical small locations. Modern approaches incorporate clusters and online data from Twitter and Google Trends to improve prediction in these small locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"read-in-the--capital-bikeshare-data\"></a>\n",
    "### Read In the Capital Bikeshare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data and set the datetime as the index.\n",
    "url = './data/bikeshare.csv'\n",
    "bikes = pd.read_csv(url, index_col='datetime', \n",
    "                    parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we used `index_col` to set an index or primary key for our data. In this case, the index of each row will be set to the value of its `datetime` field.\n",
    "\n",
    "We also ask Pandas to parse dates (if `parse_dates=True`, for the index only). So, rather than reading in a string, Pandas converts the index string to a `datetime` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-01 00:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 01:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 02:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 03:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 04:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     season  holiday  workingday  weather  temp   atemp  \\\n",
       "datetime                                                                  \n",
       "2011-01-01 00:00:00       1        0           0        1  9.84  14.395   \n",
       "2011-01-01 01:00:00       1        0           0        1  9.02  13.635   \n",
       "2011-01-01 02:00:00       1        0           0        1  9.02  13.635   \n",
       "2011-01-01 03:00:00       1        0           0        1  9.84  14.395   \n",
       "2011-01-01 04:00:00       1        0           0        1  9.84  14.395   \n",
       "\n",
       "                     humidity  windspeed  casual  registered  count  \n",
       "datetime                                                             \n",
       "2011-01-01 00:00:00        81        0.0       3          13     16  \n",
       "2011-01-01 01:00:00        80        0.0       8          32     40  \n",
       "2011-01-01 02:00:00        80        0.0       5          27     32  \n",
       "2011-01-01 03:00:00        75        0.0       3          10     13  \n",
       "2011-01-01 04:00:00        75        0.0       0           1      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the first five rows of the DataFrame.\n",
    "bikes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does each observation represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the response variable (as defined by Kaggle)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many features are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable| Description |\n",
    "|---------|----------------|\n",
    "|datetime| hourly date + timestamp  |\n",
    "|season|  1=winter, 2=spring, 3=summer, 4=fall |\n",
    "|holiday| whether the day is considered a holiday|\n",
    "|workingday| whether the day is neither a weekend nor holiday|\n",
    "|weather| See Below|\n",
    "|temp| temperature in Celsius|\n",
    "|atemp| \"feels like\" temperature in Celsius|\n",
    "|humidity| relative humidity|\n",
    "|windspeed| wind speed|\n",
    "|casual| number of non-registered user rentals initiated|\n",
    "|registered| number of registered user rentals initiated|\n",
    "|count| number of total rentals|\n",
    "\n",
    "> _Details on Weather Variable_\n",
    "\n",
    "> **1**: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "\n",
    "> **2**: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "\n",
    "> **3**: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "\n",
    "> **4**: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"count\" is a method in Pandas (and a very non-specific name), so it's best to name that column something else\n",
    "\n",
    "In general, you may want to rename columns if it is not obvious what might be stored in them. Although we will only rename the target column here, a few examples might be to rename:\n",
    "\n",
    "| old name | new name |\n",
    "| ---    | --- |\n",
    "| temp | temp_celcius\n",
    "| windspeed | windspeed_knots\n",
    "| casual | num_casual_users\n",
    "| registered | num_registered_users\n",
    "| season | season_num\n",
    "| holiday | is_holiday\n",
    "| workingday | is_workingday\n",
    "| humidity | humidity_percent\n",
    "\n",
    "Without having to check, these new names make it obvious what is stored in each column. The downside is slightly longer column names, which could affect table readability in Jupyter. It would be ideal to use very specific names in CSV files to assist others reading them. In your own code, use whatever makes sense for your work -- if you are viewing lots of Pandas tables, you may want to use shorter names. However, readable specific names are preferred in Python code since it prevents mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .rename() method to rename count to total\n",
    "bikes.rename(columns={'count':'total_rentals'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"visualizing-the-data\"></a>\n",
    "### Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to have a general feeling for what the data looks like before building a model. Ideally, before creating the model you would have some sense of which variables might matter most to predict the response. This dataset is fairly intuitive (and the purpose of this lesson is not visualization), so we will keep the visualization short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.plot(kind=\"box\", vert=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(bikes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all(bikes['casual'] + bikes['registered'] == bikes['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas scatterplot\n",
    "bikes.plot(kind='scatter', x='temp', y='total_rentals', alpha=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn scatterplot with regression line\n",
    "sns.lmplot(x='temp', y='total_rentals', \n",
    "           data=bikes, aspect=1.5, \n",
    "           scatter_kws={'alpha':0.2});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"linear-regression-basics\"></a>\n",
    "## Linear Regression Basics\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"form-of-linear-regression\"></a>\n",
    "### Form of Linear Regression\n",
    "\n",
    "Recall that each model always contains some amount of random irreducible error $\\epsilon$. So, given a prediction $\\hat{y}$, the actual $y = \\hat{y} + \\epsilon$. Below, we will assume $y$ is exactly linear.\n",
    "\n",
    "- We are often taught the formula for a line is: $y = mx + b$.\n",
    "- Note this can alternatively be written: $y = \\alpha + \\beta X$.\n",
    "\n",
    "---\n",
    "\n",
    "Here, we will generalize this to $n$ independent variables as follows:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\epsilon$\n",
    "\n",
    "- $y$ is the response.\n",
    "- $\\beta_0$ is the intercept.\n",
    "- $\\beta_1$ is the coefficient for $x_1$ (the first feature).\n",
    "- $\\beta_n$ is the coefficient for $x_n$ (the nth feature).\n",
    "- $\\epsilon$ is the _error_ term\n",
    "\n",
    "A practical example of this applied to our data might be:\n",
    "\n",
    "$total\\_rides = 20 + -2 \\cdot temp + -3 \\cdot windspeed\\ +\\ ...\\ +\\ 0.1 \\cdot registered$\n",
    "\n",
    "This equation is still called **linear** because the highest degree of the independent variables (e.g. $x_i$) is 1. Note that because the $\\beta$ values are constants, they will not be independent variables in the final model, as seen above.\n",
    "\n",
    "---\n",
    "\n",
    "The $\\beta$ values are called the **model coefficients**:\n",
    "\n",
    "- These values are estimated (or \"learned\") during the model fitting process using the **least squares criterion**.\n",
    "- Specifically, we are trying to find the line (mathematically) that minimizes the **sum of squared residuals** (or \"sum of squared errors\").\n",
    "- Once we've learned these coefficients, we can use the model to predict the response.\n",
    "\n",
    "![Estimating coefficients](./assets/estimating_coefficients.png)\n",
    "\n",
    "In the diagram above:\n",
    "\n",
    "- The black dots are the **observed values** of x and y.\n",
    "- The blue line is our **least squares line**.\n",
    "- The red lines are the **residuals**, which are the vertical distances between the observed values and the least squares line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"overview-of-supervised-learning\"></a>\n",
    "## Overview of Supervised Learning\n",
    "---\n",
    "\n",
    "![Supervised learning diagram](./assets/supervised_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"benefits-and-drawbacks-of-scikit-learn\"></a>\n",
    "### Benefits and Drawbacks of scikit-learn\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- Consistent interface to machine learning models.\n",
    "- Provides many tuning parameters but with sensible defaults.\n",
    "- Exceptional documentation.\n",
    "- Rich set of functionality for companion tasks.\n",
    "- Active community for development and support.\n",
    "\n",
    "**Potential drawbacks:**\n",
    "\n",
    "- Harder (than R) to get started with machine learning.\n",
    "- Less emphasis (than R) on model interpretability.\n",
    "    - scikit-learn tends not to run detailed statistical tests, e.g. ANOVA.\n",
    "    - For more detail on model fit, try the `statsmodels` library.\n",
    "\n",
    "Ben Lorica: [Six Reasons Why I Recommend scikit-learn](http://radar.oreilly.com/2013/12/six-reasons-why-i-recommend-scikit-learn.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"requirements-for-working-with-data-in-scikit-learn\"></a>\n",
    "### Requirements for Working With Data in scikit-learn\n",
    "\n",
    "1. Features and response should be separate objects.\n",
    "2. Features and response should be entirely numeric.\n",
    "3. Features and response should be NumPy arrays (or easily converted to NumPy arrays).\n",
    "4. Features and response should have specific shapes (outlined below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"building-a-linear-regression-model-in-sklearn\"></a>\n",
    "### Building a Linear Regression Model in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a feature matrix called X that holds a `DataFrame` with only the temp variable and a `Series` called y that has the \"total_rentals\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y.\n",
    "feature_cols = ['temp']\n",
    "X = bikes[feature_cols]\n",
    "y = bikes.total_rentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check X's type.\n",
    "print((type(X)))\n",
    "print((type(X.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check y's type.\n",
    "print((type(y)))\n",
    "print((type(y.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check X's shape (n = number of observations, m = number of features).\n",
    "print((X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check y's shape (single dimension with length n).\n",
    "# The comma indicates the datatype is a tuple.\n",
    "print((y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"scikit-learns--step-modeling-pattern\"></a>\n",
    "### scikit-learn's Four-Step Modeling Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Import the class you plan to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** \"Instantiate\" the \"estimator.\"\n",
    "\n",
    "- \"Estimator\" is scikit-learn's term for \"model.\"\n",
    "- \"Instantiate\" means \"make an instance of.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an instance of a LinearRegression object.\n",
    "lr = LinearRegression()\n",
    "type(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Created an object that \"knows\" how to do linear regression, and is just waiting for data.\n",
    "- Name of the object does not matter.\n",
    "- All parameters not specified are set to their defaults.\n",
    "- Can specify tuning parameters (aka \"hyperparameters\") during this step. \n",
    "\n",
    "To view the possible parameters, either use the `help` built-in function or evaluate the newly instantiated model, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(lr)\n",
    "lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Fit the model with data (aka \"model training\").\n",
    "\n",
    "- Model is \"learning\" the relationship between X and y in our \"training data.\"\n",
    "- Process through which learning occurs varies by model.\n",
    "- Occurs in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once a model has been fit with data, it's called a \"fitted model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Predict the response for a new observation.\n",
    "\n",
    "- New observations are called \"out-of-sample\" data.\n",
    "- Uses the information it learned during the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per future warning, one-dimensional arrays must be reshaped using the following.\n",
    "lr.predict(np.array([0]).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask the model to make two predictions, one when the `temp` is 0 and another when the `temp` is 10. To do this, our feature matrix is always a 2-D array where each row is a list of features. Since we only have a single feature, the temperature, each row will contain only a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = [[0], [10]]\n",
    "lr.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Returns a NumPy array, and we keep track of what the numbers \"mean.\"\n",
    "- Can predict for multiple observations at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we just predicted using our model is, \"If the temperature is 0 degrees, the total number of bike rentals will be ~6.046, and if the temperature is 10 degrees the total number of bike rentals will ~97.751.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"build-a-linear-regression-model\"></a>\n",
    "## Build a Linear Regression Model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's specifically make a linear regression model and look at the intercept and coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate and fit a `LinearRegression` model on X and y from the `linear_model` section of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import, instantiate, fit.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coefficients.\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Interpreting the intercept ($\\beta_0$):\n",
    "\n",
    "- It is the value of $y$ when all independent variables are 0.\n",
    "- Here, it is the estimated number of rentals when the temperature is 0 degrees Celsius.\n",
    "- **Note:** It does not always make sense to interpret the intercept. (Why?)\n",
    "\n",
    "Interpreting the \"temp\" coefficient ($\\beta_1$):\n",
    "\n",
    "- **Interpretation:** An increase of 1 degree Celcius is _associated with_ increasing the number of total rentals by $\\beta_1$.\n",
    "- Here, a temperature increase of 1 degree Celsius is _associated with_ a rental increase of 9.17 bikes.\n",
    "- This is not a statement of causation.\n",
    "- $\\beta_1$ would be **negative** if an increase in temperature was associated with a **decrease** in total rentals.\n",
    "- $\\beta_1$ would be **zero** if temperature is not associated with total rentals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"using-the-model-for-prediction\"></a>\n",
    "## Using the Model for Prediction\n",
    "---\n",
    "\n",
    "While plenty of insight can be found in reading coefficients, the most common uses of data science focus on prediction. In scikit-learn we can make predictions from a fitted model using `.predict()`, but we will also go through the calculation by hand to understand it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many bike rentals would we predict if the temperature was 25 degrees Celsius?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the intercept and coefficients of the linear model.\n",
    "\n",
    "You can search for \"sklearn linear regression\" and explore the attributes section of the documentation to learn how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually calculate the prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the predict method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"does-the-scale-of-the-features-matter\"></a>\n",
    "### Does the Scale of the Features Matter?\n",
    "\n",
    "Let's say that temperature was measured in Fahrenheit, rather than Celsius. How would that affect the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for Fahrenheit temperature.\n",
    "bikes['temp_F'] = bikes.temp * 1.8 + 32\n",
    "bikes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn scatterplot with regression line\n",
    "sns.lmplot(x='temp_F', y='total_rentals', data=bikes, aspect=1.5, scatter_kws={'alpha':0.2});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rebuild the `LinearRegression` from above using the `temp_F` features instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y.\n",
    "feature_cols = ['temp_F']\n",
    "X = bikes[feature_cols]\n",
    "y = bikes.total_rentals\n",
    "\n",
    "# Instantiate and fit.\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# Print the coefficients.\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert 25 degrees Celsius to Fahrenheit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "25 * 1.8 + 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict rentals for 77 degrees Fahrenheit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.predict(77)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** The scale of the features is irrelevant for linear regression models. When changing the scale, we simply change our interpretation of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the temp_F column.\n",
    "bikes.drop('temp_F', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"work-with-multiple-features\"></a>\n",
    "## Work With Multiple Features\n",
    "---\n",
    "\n",
    "We've demonstrated simple linear regression with one feature to gain an intuition, but the benefit of modeling is the ability to reason about hundreds of features at once. There is no limit to the number of features you can use. However, often a small set of features accounts for most of the variance (assuming there is a linear relationship at all). We will start by using four features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"visualizing-the-data-part-\"></a>\n",
    "### Visualizing the Data (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature column variables\n",
    "feature_cols = ['temp', 'season', 'weather', 'humidity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a subset of scatterplot matrix using Seaborn.\n",
    "We can use pairplot with the y_vars argument to only show relationships with the `total_rentals` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple scatterplots in Seaborn\n",
    "sns.pairplot(bikes, x_vars=feature_cols, \n",
    "             y_vars='total_rentals', kind='reg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Recreate the same functionality using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple scatterplots in Pandas\n",
    "fig, axs = plt.subplots(1, len(feature_cols), sharey=True)\n",
    "for index, feature in enumerate(feature_cols):\n",
    "    bikes.plot(kind='scatter', x=feature, y='total_rentals', \n",
    "               ax=axs[index], figsize=(16, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative way in Pandas (might take a while)\n",
    "# scatter_matrix does a pairplot of *every* column\n",
    "\n",
    "grr = pd.tools.plotting.scatter_matrix(\n",
    "    bikes[['total_rentals'] + feature_cols], \n",
    "    figsize=(15, 15), alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are you seeing anything you didn't expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the season variable using a cross-tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulation of season and month\n",
    "pd.crosstab(bikes.season, bikes.index.month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the season variable using a box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of rentals, grouped by season\n",
    "bikes.boxplot(column='total_rentals', by='season');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably:\n",
    "\n",
    "- A line can't capture a nonlinear relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at rentals over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot of rentals\n",
    "bikes.total_rentals.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does this tell us?\n",
    "\n",
    "There are more rentals in the winter than the spring, but only because the system is experiencing overall growth and the winter months happen to come after the spring months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the correlation matrix for the bikes `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix (ranges from 1 to -1)\n",
    "bikes.corr().style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a heat map to make it easier to read the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation matrix in Seaborn using a heat map.\n",
    "sns.heatmap(bikes.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What relationships do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"adding-more-features-to-the-model\"></a>\n",
    "### Adding More Features to the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, one variable explained the variance of another; however, more often than not, we will need multiple variables. \n",
    "\n",
    "- For example, a house's price may be best measured by square feet, but a lot of other variables play a vital role: bedrooms, bathrooms, location, appliances, etc. \n",
    "\n",
    "- For a linear regression, we want these variables to be largely independent of one another, but all of them should help explain the y variable.\n",
    "\n",
    "We'll work with bikeshare data to showcase what this means and to explain a concept called multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create another `LinearRegression` instance that is fit using temp, season, weather, and humidity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of features.\n",
    "feature_cols = ['temp', 'season', 'weather', 'humidity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y.\n",
    "X = bikes[feature_cols]\n",
    "y = bikes.total_rentals\n",
    "\n",
    "# Instantiate and fit.\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# Print the coefficients.\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Display the linear regression coefficient along with the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair the feature names with the coefficients.\n",
    "list(zip(feature_cols, linreg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the coefficients:\n",
    "\n",
    "- Holding all other features fixed, a 1-unit increase in temperature is associated with a rental increase of 7.86 bikes.\n",
    "- Holding all other features fixed, a 1-unit increase in season is associated with a rental increase of 22.5 bikes.\n",
    "- Holding all other features fixed, a 1-unit increase in weather is associated with a rental increase of 6.67 bikes.\n",
    "- Holding all other features fixed, a 1-unit increase in humidity is associated with a rental decrease of 3.12 bikes.\n",
    "\n",
    "Does anything look incorrect and does not reflect reality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"what-is-multicollinearity\"></a>\n",
    "## What Is Multicollinearity?\n",
    "---\n",
    "\n",
    "Multicollinearity happens when two or more features are highly correlated with each other. The problem is that due to the high correlation, it's hard to disambiguate which feature has what kind of effect on the outcome. In other words, the features mask each other. \n",
    "\n",
    "There is a second related issue called variance inflation where including correlated features increases the variability of our model and p-values by widening the standard errors. This can be measured with the variance inflation factor, which we will not cover here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With the bikeshare data, let's compare three data points: actual temperature, \"feel\" temperature, and guest ridership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "correlations = bikes[['temp', 'atemp', 'casual']].corr()\n",
    "print(correlations)\n",
    "print(sns.heatmap(correlations, cmap=cmap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Create a linear model that predicts `total_rentals` using `temp` and `atemp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of features.\n",
    "feature_cols = ['temp', 'atemp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y.\n",
    "X = bikes[feature_cols]\n",
    "y = bikes.total_rentals\n",
    "\n",
    "# Instantiate and fit.\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# Print the coefficients.\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Go back and remove either `temp` or `atemp` from the feature list. How do the coefficients change? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"how-to-select-a-model\"></a>\n",
    "## How to Select a Model\n",
    "---\n",
    "\n",
    "We can make linear models now, but how do we select the best model to use for our applications? We will offer a general procedure and a simple metric that works well in many cases. That said, it's important to keep the business context in mind and know that there are alternative metrics that can work better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"feature-selection\"></a>\n",
    "### Feature Selection\n",
    "\n",
    "How do we choose which features to include in the model? We're going to use **train/test split** (and eventually **cross-validation**).\n",
    "\n",
    "Why not use p-values or R-squared for feature selection?\n",
    "\n",
    "- Linear models rely upon a lot of assumptions (such as the features being independent), and if those assumptions are violated, p-values and R-squared are less reliable. Train/test split relies on fewer assumptions.\n",
    "- If all of the assumptions of a linear model are met, p-values suggest a coefficient that differs from zero at a level of statistical significance. This does not mean that\n",
    "    1. the feature _causes_ the response\n",
    "    2. the feature strongly _predicts_ the response. \n",
    "- Adding features to your model that are unrelated to the response will always increase the R-squared value, and adjusted R-squared does not sufficiently account for this (although, AIC and BIC do).\n",
    "- p-values and R-squared are **proxies** for our goal of generalization, whereas train/test split and cross-validation attempt to directly estimate how well the model will generalize to out-of-sample data.\n",
    "\n",
    "More generally:\n",
    "\n",
    "- There are different methodologies that can be used for solving any given data science problem, and this course follows a machine learning methodology.\n",
    "- This course focuses on general purpose approaches that can be applied to any model, rather than model-specific approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"evaluation-metrics-for-regression-problems\"></a>\n",
    "### Evaluation Metrics for Regression Problems\n",
    "\n",
    "Evaluation metrics for classification problems, such as accuracy, are not useful for regression problems. We need evaluation metrics designed for comparing continuous values.\n",
    "\n",
    "Here are three common evaluation metrics for regression problems:\n",
    "\n",
    "**Mean absolute error (MAE)** is the mean of the absolute value of the errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    "\n",
    "**Mean squared error (MSE)** is the mean of the squared errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "**Root mean squared error (RMSE)** is the square root of the mean of the squared errors:\n",
    "\n",
    "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example true and predicted response values\n",
    "true = [10, 7, 5, 5]\n",
    "pred = [8, 6, 5, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate MAE, MSE, and RMSE using imports from sklearn metrics and NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate these metrics by hand!\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "print('MAE:', metrics.mean_absolute_error(true, pred))\n",
    "print('MSE:', metrics.mean_squared_error(true, pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(true, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare these metrics:\n",
    "\n",
    "- MAE is the easiest to understand, because it's the average error.\n",
    "- MSE is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world. Also, MSE is continuous and differentiable, making it easier to use than MAE for optimization.\n",
    "- RMSE is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n",
    "\n",
    "All of these are **loss functions**, because we want to minimize them.\n",
    "\n",
    "Here's an additional example, to demonstrate how MSE/RMSE punishes larger errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same true values as above\n",
    "true = [10, 7, 5, 5]\n",
    "\n",
    "# New set of predicted values\n",
    "pred = [10, 7, 5, 13]\n",
    "\n",
    "# MAE is the same as before.\n",
    "print('MAE:', metrics.mean_absolute_error(true, pred))\n",
    "\n",
    "# MSE and RMSE are larger than before.\n",
    "print('MSE:', metrics.mean_squared_error(true, pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(true, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"comparing-models-with-traintest-split-and-rmse\"></a>\n",
    "### Comparing Models With Train/Test Split and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define a function that accepts a list of features and returns testing RMSE.\n",
    "def train_test_rmse(df, feature_cols):\n",
    "    X = df[feature_cols]\n",
    "    y = df.total_rentals\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, random_state=123)\n",
    "    \n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = linreg.predict(X_test)\n",
    "    return np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different sets of features.\n",
    "print(train_test_rmse(bikes, ['temp', 'season', 'weather', 'humidity']))\n",
    "print(train_test_rmse(bikes, ['temp', 'season', 'weather']))\n",
    "print(train_test_rmse(bikes, ['temp', 'season', 'humidity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using these as features is not allowed!\n",
    "print(train_test_rmse(bikes, ['casual', 'registered']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"comparing-testing-rmse-with-null-rmse\"></a>\n",
    "### Comparing Testing RMSE With Null RMSE\n",
    "\n",
    "Null RMSE is the RMSE that could be achieved by always predicting the mean response value. It is a benchmark against which you may want to measure your regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=123)\n",
    "\n",
    "# Create a NumPy array with the same shape as y_test.\n",
    "y_null = np.zeros_like(y_test, dtype=float)\n",
    "\n",
    "# Fill the array with the mean value of y_test.\n",
    "y_null.fill(y_test.mean())\n",
    "y_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute null RMSE.\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_null))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"feature-engineering-to-improve-performance\"></a>\n",
    "## Feature Engineering to Improve Performance\n",
    "---\n",
    "\n",
    "Machine learning models are very powerful, but they cannot automatically handle every aspect of our data. We have to explicitly modify our features to have relationships that our models can understand. In this case, we will need to pull out features to have a linear relationship with our response variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"handling-categorical-features\"></a>\n",
    "### Handling Categorical Features\n",
    "\n",
    "scikit-learn expects all features to be numeric. So how do we include a categorical feature in our model?\n",
    "\n",
    "- **Ordered categories:** Transform them to sensible numeric values (example: small=1, medium=2, large=3)\n",
    "- **Unordered categories:** Use dummy encoding (0/1). Here, each possible category would become a separate feature.\n",
    "\n",
    "What are the categorical features in our data set?\n",
    "\n",
    "- **Ordered categories:** `weather` (already encoded with sensible numeric values)\n",
    "- **Unordered categories:** `season` (needs dummy encoding), `holiday` (already dummy encoded), `workingday` (already dummy encoded)\n",
    "\n",
    "For season, we can't simply leave the encoding as 1 = spring, 2 = summer, 3 = fall, and 4 = winter, because that would imply an ordered relationship. Instead, we create multiple dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dummy variables using `get_dummies` from Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_dummies = pd.get_dummies(bikes.season, prefix='season')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the `DataFrame` of `dummies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print five random rows.\n",
    "season_dummies.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we actually only need three dummy variables (not four), and thus we'll drop the first dummy variable.\n",
    "\n",
    "Why? Because three dummies captures all of the \"information\" about the season feature, and implicitly defines spring (season 1) as the baseline level.\n",
    "\n",
    "This circles back to the concept multicollinearity, except instead of one feature being highly correlated to another, the information gained from three features is directly correlated to the fourth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_dummies.drop(season_dummies.columns[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinspect the `DataFrame` of `dummies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print five random rows.\n",
    "season_dummies.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, if you have a categorical feature with k possible values, you create k-1 dummy variables.\n",
    "\n",
    "If that's confusing, think about why we only need one dummy variable for `holiday`, not two dummy variables (`holiday_yes` and `holiday_no`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now need to concatenate the two `DataFrames` together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the original DataFrame and the dummy DataFrame (axis=0 means rows, axis=1 means columns).\n",
    "bikes_dummies = pd.concat([bikes, season_dummies], axis=1)\n",
    "\n",
    "# Print 5 random rows.\n",
    "bikes_dummies.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rerun the linear regression with dummy variables included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include dummy variables for season in the model.\n",
    "feature_cols = ['temp', 'season_2', 'season_3', 'season_4', 'humidity']\n",
    "X = bikes_dummies[feature_cols]\n",
    "y = bikes_dummies.total_rentals\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "list(zip(feature_cols, linreg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret the season coefficients? They are measured against the baseline (spring):\n",
    "\n",
    "- Holding all other features fixed, summer is associated with a rental decrease of 3.39 bikes compared to the spring.\n",
    "- Holding all other features fixed, fall is associated with a rental decrease of 41.7 bikes compared to the spring.\n",
    "- Holding all other features fixed, winter is associated with a rental increase of 64.4 bikes compared to the spring.\n",
    "\n",
    "Would it matter if we changed which season was defined as the baseline?\n",
    "\n",
    "- No, it would simply change our interpretation of the coefficients.\n",
    "\n",
    "In most situations, it is best to have the dummy that is your baseline be the category that has the largest representation.\n",
    "\n",
    "**Important:** Dummy encoding is relevant for all machine learning models, not just linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original season variable with dummy variables.\n",
    "print(train_test_rmse(bikes_dummies, ['temp', 'season', 'humidity']))\n",
    "print(train_test_rmse(bikes_dummies, ['temp', 'season_2', 'season_3', 'season_4', 'humidity']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"feature-engineering\"></a>\n",
    "### Feature Engineering\n",
    "\n",
    "See if you can create the following features:\n",
    "\n",
    "- **hour:** as a single numeric feature (0 through 23)\n",
    "- **hour:** as a categorical feature (use 23 dummy variables)\n",
    "- **daytime:** as a single categorical feature (daytime=1 from 7 a.m. to 8 p.m., and daytime=0 otherwise)\n",
    "\n",
    "Then, try using each of the three features (on its own) with `train_test_rmse` to see which one performs the best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract hour of the day to use as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes['hour'] = bikes.index.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode `hour` as a categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_dummies = pd.get_dummies(bikes.hour, prefix='hour')\n",
    "hour_dummies.drop(hour_dummies.columns[0], axis=1, inplace=True)\n",
    "bikes = pd.concat([bikes, hour_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a `daytime` variable based on hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes['daytime'] = ((bikes.hour > 6) & (bikes.hour < 21)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the root mean squared error of our various `hour` encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_test_rmse(bikes, ['hour']))\n",
    "print(train_test_rmse(bikes, bikes.columns[bikes.columns.str.startswith('hour_')]))\n",
    "print(train_test_rmse(bikes, ['daytime']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"bonus-material-regularization\"></a>\n",
    "## Bonus Material: Regularization\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regularization is a method for \"constraining\" or \"regularizing\" the size of the coefficients, thus \"shrinking\" them toward zero.\n",
    "- It reduces model variance and thus minimizes overfitting.\n",
    "- If the model is too complex, it tends to reduce variance more than it increases bias, resulting in a model that is more likely to generalize.\n",
    "\n",
    "Our goal is to locate the optimum model complexity, and thus regularization is useful when we believe our model is too complex.\n",
    "\n",
    "\n",
    "<a id=\"why-ridge-and-lasso-regression\"></a>\n",
    "\n",
    "### Reasons for Applying Lasso and Ridge\n",
    "---\n",
    "\n",
    "**Why use Ridge and Lasso Regression?**\n",
    "- The model complexity increases and we suspect that it's starting to overfit.\n",
    "- Imposing a penalty on the coefficients restricts how they grow in size, reducing the model complexity.\n",
    "\n",
    "**How should you choose between lasso regression and ridge regression?**\n",
    "\n",
    "- Lasso regression is preferred if we believe many features are irrelevant or if we prefer a sparse model because we have a very large number of features.\n",
    "- Ridge works well with with multicollinearity in features, distributing coefficients between them. Lasso arbitrarily selects a single feature among highly correlated features and reduces the rest to zero.\n",
    "- If computational performance is your primary concern, Lasso can handle many more features.\n",
    "- Ridge and Lasso handle intercept differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"how-does-regularization-work\"></a>\n",
    "### How Does Regularization Work?\n",
    "\n",
    "For a normal linear regression model, we estimate the coefficients using the least squares criterion, which minimizes the residual sum of squares (RSS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a regularized linear regression model, we minimize the sum of RSS and a \"penalty term\" that penalizes coefficient size.\n",
    "\n",
    "**Ridge regression** (or \"L2 regularization\") minimizes: $$\\text{RSS} + \\alpha \\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "**Lasso regression** (or \"L1 regularization\") minimizes: $$\\text{RSS} + \\alpha \\sum_{j=1}^p |\\beta_j|$$\n",
    "\n",
    "- $p$ is the number of features.\n",
    "- $\\beta_j$ is a model coefficient.\n",
    "- $\\alpha$ is a tuning parameter:\n",
    "    - A tiny $\\alpha$ imposes no penalty on the coefficient size, and is equivalent to a normal linear regression model.\n",
    "    - Increasing the $\\alpha$ penalizes the coefficients and thus shrinks them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lasso-and-ridge-path-diagrams\"></a>\n",
    "### Lasso and Ridge Path Diagrams\n",
    "\n",
    "A larger alpha (toward the left of each diagram) results in more regularization:\n",
    "\n",
    "- Lasso regression shrinks coefficients all the way to zero, thus removing them from the model.\n",
    "- Ridge regression shrinks coefficients toward zero, but they rarely reach zero.\n",
    "\n",
    "Source code for the diagrams: [Lasso regression](http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_lars.html) and [Ridge regression](http://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lasso and Ridge Coefficient Plots](./assets/lasso_ridge_path.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"advice-for-applying-regularization\"></a>\n",
    "### Advice for Applying Regularization\n",
    "\n",
    "**Should features be standardized?**\n",
    "\n",
    "- Yes, because otherwise, features would be penalized simply because of their scale.\n",
    "- Also, standardizing avoids penalizing the intercept, which wouldn't make intuitive sense.\n",
    "\n",
    "**How should you choose between lasso regression and ridge regression?**\n",
    "\n",
    "- Lasso regression is preferred if we believe many features are irrelevant or if we prefer a sparse model.\n",
    "- Ridge can work particularly well if there is a high degree of multicollinearity in your model.\n",
    "- If model performance is your primary concern, it is best to try both.\n",
    "- Elastic net regression is a combination of lasso regression and ridge Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"ridge-regression\"></a>\n",
    "### Ridge Regression\n",
    "\n",
    "- [Ridge](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) documentation\n",
    "- **alpha:** must be positive, increase for more regularization\n",
    "- **normalize:** scales the features (without using StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include dummy variables for season in the model.\n",
    "feature_cols = ['temp', 'atemp', 'season_2', 'season_3', 'season_4', 'humidity']\n",
    "\n",
    "X = bikes_dummies[feature_cols]\n",
    "y = bikes_dummies.total_rentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha=0 is equivalent to linear regression.\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Instantiate the model.\n",
    "#(Alpha of zero has no regularization strength, essentially a basic linear regression.)\n",
    "ridgereg = Ridge(alpha=0, normalize=True)\n",
    "\n",
    "# Fit the model.\n",
    "ridgereg.fit(X_train, y_train)\n",
    "\n",
    "# Predict with fitted model.\n",
    "y_pred = ridgereg.predict(X_test)\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients for a non-regularized linear regression\n",
    "list(zip(feature_cols, ridgereg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret these coefficients we need to convert them back to original units, which is a reason to do normalization by hand. However, in this form the coefficients have a special meaning. The intercept is now the average of our outcome, and the magnitude of each coefficient in the model is a measure of how important it is in the model. We call this feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try alpha=0.1.\n",
    "ridgereg = Ridge(alpha=0.1, normalize=True)\n",
    "ridgereg.fit(X_train, y_train)\n",
    "y_pred = ridgereg.predict(X_test)\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the coefficients.\n",
    "list(zip(feature_cols, ridgereg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the MSE barely improved, we can see there are significant changes in the weight of our coefficients.  Particularly `season_2` whose coefficient has greatly decreased toward 0.\n",
    "\n",
    "Fitting and using a Lasso Regression in scikit-learn is very similar.  \n",
    "\n",
    "In addition to the typical [lasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) and [ridge](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) there is a third type of regression, [Elastic Net](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) which combines the penalties of the ridge and lasso methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"comparing-linear-regression-with-other-models\"></a>\n",
    "## Comparing Linear Regression With Other Models\n",
    "\n",
    "Advantages of linear regression:\n",
    "\n",
    "- Simple to explain.\n",
    "- Highly interpretable.\n",
    "- Model training and prediction are fast.\n",
    "- No tuning is required (excluding regularization).\n",
    "- Features don't need scaling.\n",
    "- Can perform well with a small number of observations.\n",
    "- Well understood.\n",
    "\n",
    "Disadvantages of linear regression:\n",
    "\n",
    "- Presumes a linear relationship between the features and the response.\n",
    "- Performance is (generally) not competitive with the best supervised learning methods due to high bias.\n",
    "- Can't automatically learn feature interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
